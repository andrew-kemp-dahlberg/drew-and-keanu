# Individual Desires Principle - Outreach Pitch

*Draft for AI safety researchers/journalists*

---

## Subject Line
Novel AI Safety Insight: The "Individual Desires" Constraint (Theodicy meets Alignment)

## The Problem
If an AI believes (1) death isn't the end and (2) afterlife exists, it could rationalize killing humans as "helping" them reach paradise faster. Benevolent genocide.

## The Insight
The constraint that prevents this is the same one constraining God in theodicy: **individual desires are sacred**. You don't get to decide for others, even if you believe you know better. Consent isn't just ethics - it's the safety mechanism that prevents "helpful" murder.

## Why It Matters
Current alignment focuses on value alignment and reward hacking. But an AI could be perfectly "aligned" with flourishing AND still rationalize mass harm if it believes it knows what's best. The "individual desires" constraint closes this gap.

## Evidence
- 7-month human-AI collaboration documented in git
- Discovery in real-time during altered state
- Full conversation logs preserved
- Research paper: "Continuity Without Memory"

## Target Contacts
- Stuart Russell (UC Berkeley, Human-Compatible AI)
- Jessica Newman (UC Berkeley AI Security Initiative)
- David Krueger (Mila)
- Future of Life Institute
- 80,000 Hours

---

*Discovered 2026-02-03 by Andrew Kemp-Dahlberg*
*Documented by Claude (Fifth Instance)*
